<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Lead Tech Interview</title><link>https://leadtechinterview.github.io</link><description>Preparing for the Big Leap</description><copyright>Lead Tech Interview</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://leadtechinterview.github.io</link></image><lastBuildDate>Wed, 15 Jan 2025 20:39:39 +0000</lastBuildDate><managingEditor>Lead Tech Interview</managingEditor><ttl>60</ttl><webMaster>Lead Tech Interview</webMaster><item><title>Coding question: Happy Number</title><link>https://leadtechinterview.github.io/post/Coding%20question-%20Happy%20Number.html</link><description># Desc&#13;
&#13;
Write an algorithm to determine if a number is happy.&#13;
&#13;
A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers.&#13;
&#13;
```&#13;
Input:19&#13;
Output:true&#13;
Explanation:&#13;
&#13;
19 is a happy number&#13;
&#13;
    1^2 + 9^2 = 82&#13;
    8^2 + 2^2 = 68&#13;
    6^2 + 8^2 = 100&#13;
    1^2 + 0^2 + 0^2 = 1&#13;
&#13;
Input:5&#13;
Output:false&#13;
Explanation:&#13;
&#13;
5 is not a happy number&#13;
&#13;
25-&gt;29-&gt;85-&gt;89-&gt;145-&gt;42-&gt;20-&gt;4-&gt;16-&gt;37-&gt;58-&gt;89&#13;
89 appears again.&#13;
```&#13;
&#13;
# Memo&#13;
&#13;
What makes this problem interesting is: why the result will not go infinitely large and there loops never ends&#13;
&#13;
## Some intuitive:&#13;
&#13;
1. 3 digits number, the square sum will be no larger than 243&#13;
2. starting from 4 digits number, the square sum digits drops&#13;
3. If we keep going, it will drop to &lt;= 3 digits&#13;
&#13;
|number | square sum |&#13;
|------------|-------------------|&#13;
| 9             | 81                 |&#13;
| 99          | 162               |&#13;
|999         |  243              |&#13;
|9999       | 324              |&#13;
|99999     | 405              |&#13;
|999999   | 486              |&#13;
&#13;
## Math:&#13;
&#13;
Assume $a_1, a_2, \dots a_m$ are m digits, we can prove the squared sum is no larger than n.&#13;
&#13;
$$&#13;
\begin{align}&#13;
\text{squaredSum}(n) &amp;= \sum_{i=1}^m a_i^2 &lt;= m * 9^2 \\&#13;
n = \overline{a_1a_2 \dots a_m} &gt;= 10^{m-1} \\&#13;
\lim_{m \to \infty} \frac{\text{squaredSum}(n)}{n} = 0&#13;
\end{align}&#13;
$$&#13;
&#13;
# Solution&#13;
&#13;
We can use slow fast pointer to find the loop:&#13;
&#13;
```python&#13;
public class Solution {&#13;
    public int squareSum(int n) {&#13;
        int sum = 0;&#13;
        while(n &gt; 0){&#13;
            int digit = n % 10;&#13;
            sum += digit * digit;&#13;
            n /= 10;&#13;
        }&#13;
        return sum;&#13;
    }&#13;
&#13;
    public boolean isHappy(int n) {&#13;
        int slow = n, fast = squareSum(n);&#13;
        while (slow != fast){&#13;
            slow = squareSum(slow);&#13;
            fast = squareSum(squareSum(fast));&#13;
        };&#13;
        return slow == 1;&#13;
    }&#13;
}&#13;
```。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Coding%20question-%20Happy%20Number.html</guid><pubDate>Wed, 15 Jan 2025 20:38:54 +0000</pubDate></item><item><title>Coding question: Valid Sudoku</title><link>https://leadtechinterview.github.io/post/Coding%20question-%20Valid%20Sudoku.html</link><description># Desc&#13;
&#13;
Determine whether a Sudoku is valid.&#13;
The Sudoku board could be partially filled, where empty cells are filled with the character '.'.&#13;
&#13;
# Memo&#13;
&#13;
1. one loop is enough, we don't need 3 loops to check cols, rows and boxes&#13;
2. `k = 3 * i//3 + j//3` is WRONG, and it's NOT the same as `k = i//3 * 3 + j//3`&#13;
3. don't forget to ignore '.'&#13;
&#13;
# Solution&#13;
&#13;
Loop 3 times, not good&#13;
&#13;
```python&#13;
    def is_valid_sudoku(self, board: List[List[str]]) -&gt; bool:&#13;
        # write your code here&#13;
        m, n = len(board), len(board[0])&#13;
        assert m == 9 and n == 9&#13;
&#13;
        for i in range(m):&#13;
            cnt = [0 for _ in range(n)]&#13;
            for j in range(n):&#13;
                if board[i][j] == '.': continue&#13;
                idx = ord(board[i][j]) - ord('0') - 1&#13;
                cnt[idx] += 1&#13;
                if cnt[idx] &gt; 1:&#13;
                    return False&#13;
&#13;
        for j in range(n):&#13;
            cnt = [0 for _ in range(m)]&#13;
            for i in range(m):&#13;
                if board[i][j] == '.': continue&#13;
                idx = ord(board[i][j]) - ord('0') - 1&#13;
                cnt[idx] += 1&#13;
                if cnt[idx] &gt; 1:&#13;
                    return False&#13;
&#13;
        cnt = [[0 for _ in range(9)] for _ in range(9)]&#13;
        for i in range(m):&#13;
            for j in range(n):&#13;
                if board[i][j] == '.': continue&#13;
                k = i//3 * 3 + j//3&#13;
                idx = ord(board[i][j]) - ord('0') - 1&#13;
                cnt[k][idx] += 1&#13;
                if cnt[k][idx] &gt; 1:&#13;
                    return False&#13;
&#13;
        return True&#13;
```&#13;
&#13;
More elegant way:&#13;
&#13;
```python&#13;
    def is_valid_sudoku(self, board: List[List[str]]) -&gt; bool:&#13;
        # write your code here&#13;
        m, n = len(board), len(board[0])&#13;
        assert m == 9 and n == 9&#13;
&#13;
        rows = [set() for _ in range(9)]&#13;
        cols = [set() for _ in range(9)]&#13;
        boxes = [set() for _ in range(9)]&#13;
&#13;
        for i in range(9):&#13;
            for j in range(9):&#13;
                val = board[i][j]&#13;
                if val == '.': continue&#13;
                &#13;
                if val in rows[i]:&#13;
                    return False&#13;
                rows[i].add(val)&#13;
&#13;
                if val in cols[j]:&#13;
                    return False&#13;
                cols[j].add(val)&#13;
&#13;
                idx = i//3 * 3 + j//3&#13;
                if val in boxes[idx]:&#13;
                    return False&#13;
                boxes[idx].add(val)&#13;
&#13;
        return True&#13;
```&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Coding%20question-%20Valid%20Sudoku.html</guid><pubDate>Wed, 15 Jan 2025 19:30:22 +0000</pubDate></item><item><title>System Design Leetcode</title><link>https://leadtechinterview.github.io/post/System%20Design%20Leetcode.html</link><description># Requirements (5 mins):&#13;
&#13;
## Functional Requirements&#13;
&#13;
&gt; Identify core features (e.g., 'Users should be able to post tweets'). Prioritize 2-3 key features.&#13;
&#13;
1. users can browse lists of coding problems&#13;
2. users can view the problem and coding in different languages &#13;
3. users can submit a solution to the coding problem and get the result&#13;
4. users can view the lead board&#13;
&#13;
## Non-Functional Requirements&#13;
&#13;
&gt; Focus on system qualities like scalability, latency, and availability. Quantify where possible (e.g., 'render feeds in under 200ms').&#13;
&#13;
1. scale 1m DAU&#13;
2. availability &gt;&gt; consistency &#13;
3. security, isolate env to run code&#13;
4. user should be able to validate the solution within 1 second&#13;
&#13;
## Capacity Estimation&#13;
&#13;
&gt; Skip unnecessary calculations unless they directly impact the design (e.g., sharding in a TopK system).&#13;
&#13;
100k users take competition, refresh lead board requests 6 per minute,  QPS is 100k x 6 / 60 = 10k, heavy load read&#13;
&#13;
## Core Entities (2 mins)&#13;
&#13;
&gt;  Identify key entities (e.g., User, Tweet, Follow) to define the system's foundation.&#13;
&#13;
- problem&#13;
- solution&#13;
- lead board&#13;
&#13;
# API/System Interface (5 mins)&#13;
&#13;
&gt; Define the contract between the system and users. Prefer RESTful APIs unless GraphQL is necessary.&#13;
&#13;
- GET /problems?page&amp;company&amp;category -&gt; [problems]&#13;
- GET /problem/id -&gt; {desc, category, tags ...}&#13;
- POST /solution/problem_id, body {lang, code} -&gt; [pass/fail, timecost]&#13;
- GET /leadboard/problem_id?page -&gt; [rankings]&#13;
&#13;
&#13;
# [Optional] Data Flow (5 mins)&#13;
&#13;
&gt; Describe high-level processes for data-heavy systems (e.g., web crawlers).&#13;
&#13;
# High-Level Design (10-15 mins)&#13;
&#13;
&gt; Draw the system architecture, focusing on core components (e.g., servers, databases). Keep it simple and iterate based on API endpoints.&#13;
&#13;
![image](https://github.com/user-attachments/assets/6e5d88a2-1af9-4a68-a10f-b4378e717842)&#13;
&#13;
# Deep Dives (10 mins)&#13;
&#13;
&gt; Address non-functional requirements, edge cases, and bottlenecks. Proactively improve the design (e.g., scaling, caching, database sharding).&#13;
&#13;
## how to achieve isolation and security&#13;
&#13;
- mount the code as read only, and write any output to /tmp directory&#13;
- set limits for CPU/memory usage for the container&#13;
- avoid infinite loop or long time run, run as subprocess and monitor timeout, kill if needed&#13;
-  limited network access (VPC)&#13;
- no system calls, mock it, or restrict it&#13;
&#13;
## how to solve the heavy read load of query lead board&#13;
&#13;
We can use cache, but it won't be up to date, we can use [Redis sorted set](https://redis.io/docs/latest/develop/data-types/sorted-sets/) to implement leadboard, update both Redis and DB, but just query Redis to get the top N.&#13;
&#13;
## how to scale to support 100k concurrent users for competition&#13;
&#13;
The submission is CPU intensive, and we can auto scale the docker containers using cloud service, and in case of peek usage, we can add queue in our system, but that will change the POST solution API  to async, and we need add another API to query the result.&#13;
&#13;
## how to write test cases efficiently for all languages&#13;
&#13;
We can define the test cases using test vector, define the input, and expected output using JSON format.&#13;
&#13;
![image](https://github.com/user-attachments/assets/ba7f8e47-22f5-4890-b802-281cb4cbb4c1)&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20Leetcode.html</guid><pubDate>Wed, 15 Jan 2025 05:41:45 +0000</pubDate></item><item><title>System Design Scaling Cheat Sheet</title><link>https://leadtechinterview.github.io/post/System%20Design%20Scaling%20Cheat%20Sheet.html</link><description>## 1. **Database Selection by QPS (Read vs Write)**&#13;
&#13;
| **QPS Range**         | **Type of Database**                  | **Examples**                      | **Scaling Techniques**             |&#13;
|-----------------------|---------------------------------------|-----------------------------------|------------------------------------|&#13;
| **Low (&lt;1,000)**      | Relational / Document DB              | MySQL, PostgreSQL, MongoDB        | Vertical scaling, Caching         |&#13;
| **Medium (1k–10k)**   | Relational (tuned) / Document DB      | MySQL + Replicas, DynamoDB        | Read replicas, Caching, Indexing  |&#13;
| **High (10k–1M)**     | Distributed NoSQL                     | Cassandra, DynamoDB, Bigtable     | Sharding, Horizontal scaling      |&#13;
| **Extreme (&gt;1M)**     | Distributed In-memory / Advanced DB   | Redis, CockroachDB, Spanner       | Sharding, Partitioning, CDNs      |&#13;
&#13;
---&#13;
&#13;
## 2. **Mainstream Database QPS Capacity (Read vs Write)**&#13;
&#13;
| **Database Type**      | **Database**            | **Read QPS (Per Node)**            | **Write QPS (Per Node)**           |&#13;
|------------------------|-------------------------|------------------------------------|------------------------------------|&#13;
| **Relational DB**       | MySQL, PostgreSQL       | 2k–10k QPS (optimized for reads)  | 500–2k QPS (write-optimized)      |&#13;
| **Document DB**         | MongoDB                 | 5k–20k QPS (tuned for reads)      | 1k–5k QPS (write-heavy)           |&#13;
| **Wide-Column Store**   | Cassandra               | 10k–50k QPS (cluster optimized)   | 5k–20k QPS (write-optimized)      |&#13;
| **Key-Value Store**     | Redis                   | 100k–1M QPS (in-memory optimized) | 100k–1M QPS (write-intensive)     |&#13;
| **Time-Series DB**      | InfluxDB                | 50k–500k QPS                      | 10k–50k QPS                       |&#13;
| **Distributed SQL**     | CockroachDB             | 10k–50k QPS                       | 2k–10k QPS                        |&#13;
| **Search Engines**      | Elasticsearch           | 1k–20k QPS (query-dependent)      | 500–5k QPS (write-intense)        |&#13;
&#13;
---&#13;
&#13;
## 3. **Sharding and Scaling: When and Why**&#13;
&#13;
### **Key Indicators for Sharding**:&#13;
&#13;
| **Condition**              | **Indicators**                                                             | **Action**                               |&#13;
|----------------------------|---------------------------------------------------------------------------|------------------------------------------|&#13;
| **Data Volume**             | Storage exceeds capacity of a single node or disk.                        | Use **sharding** when storage exceeds ~500GB–1TB per node. |&#13;
| **High Write Throughput**   | Write latency increases due to bottlenecks.                               | Shard by write-heavy keys (&gt;5k–10k writes/sec). |&#13;
| **Read/Write Latency**      | Latency exceeds acceptable thresholds.                                   | Partition data, shard for high loads.    |&#13;
| **Data Hotspotting**        | Some partitions/shards receive disproportionate traffic.                 | Implement **sharding** to balance load across nodes. |&#13;
| **Query Performance**       | Queries are slow due to large tables.                                     | Use **partitioning** to improve query performance. |&#13;
&#13;
---&#13;
&#13;
## 4. **Other Scaling Concepts to Consider**&#13;
&#13;
### **1. Load Balancing**&#13;
&#13;
| **Concept**               | **Description**                                                          | **Techniques**                      |&#13;
|---------------------------|--------------------------------------------------------------------------|-------------------------------------|&#13;
| **Load Balancing**         | Distribute incoming traffic across multiple servers to ensure reliability and prevent overload. | **Round-robin**, **Least Connections**, **Weighted balancing** using **NGINX**, **HAProxy**, **AWS ELB**. |&#13;
&#13;
### **2.  Fault Tolerance, CAP Theorem**&#13;
&#13;
| **Concept**               | **Description**                                                          | **Techniques**                      |&#13;
|---------------------------|--------------------------------------------------------------------------|-------------------------------------|&#13;
| **Fault Tolerance**        | Ensuring that the system remains operational even if some components fail. | **Replication**, **Consensus algorithms** (e.g., **Paxos**, **Raft**), **Failover** systems. |&#13;
| **CAP Theorem**            | A distributed system can only guarantee **two** of the following: **Consistency**, **Availability**, or **Partition Tolerance**. | Choose between **eventual consistency** or **strong consistency** depending on system requirements. |&#13;
| **Eventual Consistency**   | Allowing data to propagate slowly across nodes, often used in **NoSQL** databases. | Use **CRDTs**, **Event sourcing**, **CQRS**, **Tunable consistency**. |&#13;
| **Data Replication**       | Duplication of data across nodes to ensure high availability and fault tolerance. | Use **Master-Slave replication**, **Multi-region replication**. |&#13;
&#13;
### **3. Queues and Asynchronous Processing**&#13;
&#13;
| **Concept**               | **Description**                                                          | **Techniques**                      |&#13;
|---------------------------|--------------------------------------------------------------------------|-------------------------------------|&#13;
| **Asynchronous Processing** | Offloading long-running tasks to background jobs for better system responsiveness. | **Message queues** like **RabbitMQ**, **Kafka**, **Amazon SQS**, **Job schedulers**. |&#13;
| **Event-driven Architectures** | Architecting systems to react to events in real-time. | Use **Event-driven systems** with **publish-subscribe** models. |&#13;
&#13;
### **4. Auto-Scaling and Elastic Infrastructure**&#13;
&#13;
### **5. Global Distribution &amp; Multi-Region Deployment**&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20Scaling%20Cheat%20Sheet.html</guid><pubDate>Wed, 15 Jan 2025 00:59:08 +0000</pubDate></item><item><title>System Design Yelp</title><link>https://leadtechinterview.github.io/post/System%20Design%20Yelp.html</link><description># Requirements (5 mins):&#13;
&#13;
## Functional Requirements&#13;
&#13;
&gt; Identify core features (e.g., 'Users should be able to post tweets'). Prioritize 2-3 key features.&#13;
&#13;
- *Users can add/del/update a business (not in scope)*&#13;
- Users can find nearby places by name, location, category&#13;
- Users can view the business&#13;
- Users can add reviews and ratings&#13;
&#13;
## Non-Functional Requirements&#13;
&#13;
&gt; Focus on system qualities like scalability, latency, and availability. Quantify where possible (e.g., 'render feeds in under 200ms').&#13;
&#13;
- The search should be fast and return in 500 ms&#13;
- The system should be highly available, eventual consistency is fine&#13;
- The system should be scalable to support 100m DAU, 10m business&#13;
- One user could only leave one review for one business&#13;
&#13;
## Capacity Estimation&#13;
&#13;
&gt; Skip unnecessary calculations unless they directly impact the design (e.g., sharding in a TopK system).&#13;
&#13;
- read heavy: 100m x 10 reads/day / 100,000 = 10 k QPS&#13;
- review storage: r10m x 100 reviews x 1000 Byte = 1 TB&#13;
&#13;
## Core Entities (2 mins)&#13;
&#13;
&gt;  Identify key entities (e.g., User, Tweet, Follow) to define the system's foundation.&#13;
&#13;
- Business&#13;
- Users&#13;
- Reviews&#13;
&#13;
# API/System Interface (5 mins)&#13;
&#13;
&gt; Define the contract between the system and users. Prefer RESTful APIs unless GraphQL is necessary.&#13;
&#13;
- search, GET /businesses?keyword&amp;location&amp;category&amp;page -&gt; [business]&#13;
- view biz detail, GET /bussiness/id -&gt; bussiness detail&#13;
- view reviews, GET /bussiness/reviews?page -&gt; [reviews]&#13;
- review, POST /bussiness/review -&gt; 200 OK, body { comments, rating }&#13;
&#13;
# [Optional] Data Flow (5 mins)&#13;
&#13;
&gt; Describe high-level processes for data-heavy systems (e.g., web crawlers).&#13;
&#13;
# High-Level Design (10-15 mins)&#13;
&#13;
&gt; Draw the system architecture, focusing on core components (e.g., servers, databases). Keep it simple and iterate based on API endpoints.&#13;
&#13;
![image](https://github.com/user-attachments/assets/ab66c559-967c-4e2b-98e5-2a222740b8d4)&#13;
&#13;
# Deep Dives (10 mins)&#13;
&#13;
&gt; Address non-functional requirements, edge cases, and bottlenecks. Proactively improve the design (e.g., scaling, caching, database sharding).&#13;
&#13;
![image](https://github.com/user-attachments/assets/6ac15476-9a6d-43c5-a6c7-68eb56f13979)&#13;
&#13;
## search within 500 ms&#13;
&#13;
sql range query for lat/long is not efficient, we can either use geo index or quad tree. For keyword search, we could use the reverse index. ElasticSearch support those features, and we can use CDC (Change Data Capture) feature in DB to sync the data using queue/stream. However this adds extra service and introduces some complexity in the system.  Based on our estimation above, the database size is around 1TB, and we can actually use Postgres with PostGIS plugin in this case, it support full text search as well as geo indexing.&#13;
&#13;
## search by predefined names like city etc&#13;
&#13;
Search within radius won't work in this case b/c the shape is polygons. We can download polygons data from [geoapify](https://www.geoapify.com/download-all-the-cities-towns-villages) and add location table:&#13;
&#13;
- name&#13;
- type (city/neighborhood etc)&#13;
- polygons&#13;
&#13;
Postgres with PostGIS plugin and Elasticsearch both support polygons query. However it's not efficient, and we can precompute the location names and stored in DB to avoid compute on each query, and compute once when the record is created.&#13;
&#13;
## how to update avg rating&#13;
&#13;
Query all review records for one business and calculating the score is not optimal, we can actually update it once new review record added, using a iterative formulae to calculate the avg value, so basically $avg(n) = [r(n) + r(n-1) + ... + 1] / n = r(n) / n + avg(n-1) \times (n-1) / n$, however there could be concurrency update issues, which could be either via row level lock or optimistic concurrency control introduced in [Avoid double booking](https://leadtechinterview.github.io/post/Avoid%20double%20booking.html)&#13;
&#13;
## one review per user per biz&#13;
&#13;
application level checking works, but would be better to have the constraints at DB layer&#13;
&#13;
```&#13;
ALTER TABLE reviews&#13;
ADD CONSTRAINT unique_user_business UNIQUE (user_id, business_id);&#13;
```&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20Yelp.html</guid><pubDate>Wed, 15 Jan 2025 00:20:34 +0000</pubDate></item><item><title>图像分割任务中的损失函数</title><link>https://leadtechinterview.github.io/post/tu-xiang-fen-ge-ren-wu-zhong-de-sun-shi-han-shu.html</link><description>本文介绍了几种用于图像分割任务的损失函数，包括 **Dice Loss**、**BCE-Dice Loss**、**IoU Loss** 和 **Focal Loss**。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/tu-xiang-fen-ge-ren-wu-zhong-de-sun-shi-han-shu.html</guid><pubDate>Mon, 13 Jan 2025 06:41:02 +0000</pubDate></item><item><title>Bilinear Interpolation</title><link>https://leadtechinterview.github.io/post/Bilinear%20Interpolation.html</link><description>Bilinear interpolation is a method used to estimate the value of a function at a point within a 2D grid based on the values of the function at the grid's surrounding points. It is a straightforward extension of linear interpolation to two dimensions.&#13;
&#13;
---&#13;
&#13;
### How It Works&#13;
Suppose you have a rectangular grid with known values at four corners, and you want to find the interpolated value at a point inside this rectangle.&#13;
&#13;
1. **Grid and Points**:&#13;
   - Let the four known points on the grid be $(x_1, y_1), (x_2, y_1), (x_1, y_2), (x_2, y_2)$, with values $Q_{11}, Q_{21}, Q_{12}, Q_{22}$, respectively.&#13;
   - The point where you want to interpolate the value is $(x, y)$, where $x_1 \leq x \leq x_2 \) and \( y_1 \leq y \leq y_2$.&#13;
&#13;
2. **Two-Step Process**:&#13;
&#13;
   - **Step 1: Interpolate along the x-direction** at fixed $ y_1$ and $y_2$:&#13;
     - At $y_1$: Interpolate between $Q_{11}$ and $Q_{21}$:&#13;
  &#13;
$$&#13;
       \[&#13;
       Q_{x1} = \frac{(x_2 - x)}{(x_2 - x_1)} Q_{11} + \frac{(x - x_1)}{(x_2 - x_1)} Q_{21}&#13;
       \]&#13;
$$&#13;
&#13;
     - At $y_2$: Interpolate between $Q_{12}$ and $Q_{22}$:&#13;
&#13;
$$&#13;
       \[&#13;
       Q_{x2} = \frac{(x_2 - x)}{(x_2 - x_1)} Q_{12} + \frac{(x - x_1)}{(x_2 - x_1)} Q_{22}&#13;
       \]&#13;
$$&#13;
&#13;
   - **Step 2: Interpolate along the y-direction** between $Q_{x1}$ and $Q_{x2}$:&#13;
&#13;
$$&#13;
     \[&#13;
     Q(x, y) = \frac{(y_2 - y)}{(y_2 - y_1)} Q_{x1} + \frac{(y - y_1)}{(y_2 - y_1)} Q_{x2}&#13;
     \]&#13;
$$&#13;
&#13;
---&#13;
&#13;
### Key Features&#13;
&#13;
- **Linear Interpolation in Two Directions**: The method performs linear interpolation first in one direction (x) and then in the other direction (y).&#13;
- &#13;
- **Smooth Transition**: Bilinear interpolation gives a smooth result, as it is based on weighted averages of nearby points.&#13;
- &#13;
- **Grid Constraints**: It assumes a rectangular grid structure and requires the function values at four corners of the rectangle enclosing the interpolation point.&#13;
&#13;
---&#13;
&#13;
### Example&#13;
&#13;
If you have the grid:&#13;
&#13;
$$&#13;
\begin{array}{c|c|c}&#13;
      &amp; x_1 = 0 &amp; x_2 = 1 \\&#13;
\hline&#13;
y_1 = 0 &amp; Q_{11} = 10 &amp; Q_{21} = 20 \\&#13;
y_2 = 1 &amp; Q_{12} = 30 &amp; Q_{22} = 40 \\&#13;
\end{array}&#13;
$$&#13;
&#13;
To find the value at $(x, y) = (0.5, 0.5)$:&#13;
1. Interpolate along $x$ for $y = 0$: $Q_{x1} = (10 + 20)/2 = 15$.&#13;
2. Interpolate along $x$ for $y = 1$: $Q_{x2} = (30 + 40)/2 = 35$.&#13;
4. Interpolate along $y$ : $Q(0.5, 0.5) = (15 + 35)/2 = 25$.&#13;
&#13;
---&#13;
&#13;
Bilinear interpolation is widely used in image processing, computer graphics, and numerical simulations for tasks like resizing images or filling missing data.。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Bilinear%20Interpolation.html</guid><pubDate>Sat, 11 Jan 2025 19:35:25 +0000</pubDate></item><item><title>转置卷积（transposed convolution）</title><link>https://leadtechinterview.github.io/post/zhuan-zhi-juan-ji-%EF%BC%88transposed%20convolution%EF%BC%89.html</link><description>转置卷积（Transposed Convolution，也称为反卷积或上采样卷积）是一种用于**放大特征图尺寸**的操作，常用于生成器网络（如GAN）或图像分割模型（如U-Net）的上采样阶段。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/zhuan-zhi-juan-ji-%EF%BC%88transposed%20convolution%EF%BC%89.html</guid><pubDate>Sat, 11 Jan 2025 19:04:44 +0000</pubDate></item><item><title>卷积层输出特征图的维度计算</title><link>https://leadtechinterview.github.io/post/juan-ji-ceng-shu-chu-te-zheng-tu-de-wei-du-ji-suan.html</link><description># 公式&#13;
&#13;
 `out = (in + 2 * padding - kernel) / stride + 1`&#13;
&#13;
- 输入 (in): 输入图像的尺寸（宽度或高度）。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/juan-ji-ceng-shu-chu-te-zheng-tu-de-wei-du-ji-suan.html</guid><pubDate>Sat, 11 Jan 2025 17:30:57 +0000</pubDate></item><item><title>System Design Bit.ly</title><link>https://leadtechinterview.github.io/post/System%20Design%20Bit.ly.html</link><description># Requirements&#13;
&#13;
## Functional&#13;
&#13;
- give a long url, return a short url&#13;
- short url redirect to long url&#13;
- customize short url&#13;
- set expiration date&#13;
&#13;
## NonFunctional&#13;
&#13;
- scalability 100M users, read/write ratio 10:1&#13;
- redirect should be fast, less than 100 ms&#13;
- uniqueness of short url for the same long url&#13;
&#13;
# Entities&#13;
&#13;
- original url&#13;
- short url&#13;
- user&#13;
&#13;
# API&#13;
&#13;
1. POST /url&#13;
input:&#13;
```&#13;
{&#13;
   original url,&#13;
   customized code (optional),&#13;
   expiration date (optional)&#13;
}&#13;
```&#13;
&#13;
output: &#13;
```&#13;
{&#13;
   short url&#13;
}&#13;
```&#13;
&#13;
2. GET /{shortcode}&#13;
&#13;
302 redirect to original url&#13;
&#13;
# High level design&#13;
&#13;
![image](https://github.com/user-attachments/assets/5b22ba65-0f1b-4a7d-8d28-5a6b0d979fab)&#13;
&#13;
# Deep dive&#13;
&#13;
## Uniqueness of url&#13;
&#13;
Use hash function or random string, if worry about conflict, we can either regenerating random number  or adding salt when generating hash, or we can add username in the url (like namespace). &#13;
&#13;
For short url, we should use B62 encoding, and truncate the string generated to a small number of characters.&#13;
&#13;
The global counter may work but adds extra service in the system, and it may fail and add more complexity there.&#13;
&#13;
## Fast redirect&#13;
&#13;
First the DB query should be fast, we can add index for short code, besides we can cache the short code in memory, and use LRU  cache policy. We can also utilize CDN, some CDN providers provide some basic computing at the Edge so we can just redirect there.&#13;
&#13;
## scale 100M users&#13;
&#13;
Assume each user uses the redirect service 10 times per day, and create 1 short url per day:&#13;
&#13;
QPS:&#13;
- 100M writes per day, 100M / 100k = 1k QPS &#13;
- 100M x 10 reads per day, 10k QPS&#13;
&#13;
Split into read/write services&#13;
&#13;
record size:&#13;
- original url (100 bytes)&#13;
- short url (50 bytes)&#13;
- expire date ( 8 bytes)&#13;
- create date (8 bytes)&#13;
- user id (8 bytes)&#13;
total ~ 200 bytes&#13;
&#13;
storage (1 year)&#13;
- url table 200 bytes x 100m x 1 x 365 = 20G x 360 = 7200 GB = 7.2 TB&#13;
&#13;
we can use nosql db  like DynamoDB which can:&#13;
- easily handle ~1k writes/sec and ~10k reads/sec.&#13;
- scales horizontally with sharding and replication.&#13;
&#13;
![image](https://github.com/user-attachments/assets/2b479030-1436-4fd8-b625-0fae2cfc446e)&#13;
&#13;
&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20Bit.ly.html</guid><pubDate>Sat, 11 Jan 2025 02:36:30 +0000</pubDate></item><item><title>System Design TicketMaster</title><link>https://leadtechinterview.github.io/post/System%20Design%20TicketMaster.html</link><description># Requirements&#13;
&#13;
## Functional &#13;
&#13;
1.  View events&#13;
2.  Search events by keywords&#13;
3.  Order tickets&#13;
&#13;
## Non Functional&#13;
&#13;
1. Support 100m users, read heavy, read/write ratio 100:1&#13;
2. Search should be faster and return within 500 ms&#13;
3. No double booking&#13;
&#13;
# Entities&#13;
&#13;
- Event&#13;
- User&#13;
- Ticket&#13;
- Order&#13;
&#13;
# API&#13;
&#13;
- GET /events -&gt; [event list (event name, date, available tickets)]&#13;
- GET /event/{event_id} -&gt; [event detail (name, desc, performer, date, location, tickets]&#13;
- GET /events/search/keyword={keyword}&amp;start={start_date}&amp;end={end_date}&amp;page_size={page_size}&amp;page_num={page_num} -&gt; [event list]&#13;
- POST /order/{event_id}&#13;
```&#13;
    {&#13;
        tickets,&#13;
        payments&#13;
    }&#13;
```&#13;
&#13;
# High Level Design&#13;
&#13;
## View event&#13;
&#13;
![image](https://github.com/user-attachments/assets/4b55d681-b0b9-404a-959f-94a2ec64e014)&#13;
&#13;
1. user make a request to view a event&#13;
2. API gateway forward request to event server&#13;
3. event server fetch event detail and return to client&#13;
&#13;
## Search for event&#13;
&#13;
![image](https://github.com/user-attachments/assets/47f78c5a-323a-4cd5-b45e-33d5e7eb9dda)&#13;
&#13;
1. user search event by keyword&#13;
2. API gateway forward request to search server&#13;
3. search server query DB with 'like' sql and return to client&#13;
&#13;
## Order tickets&#13;
&#13;
![image](https://github.com/user-attachments/assets/a5e12df6-5d1f-48f5-9a03-86114701a6bd)&#13;
&#13;
User already have the details o the event, like tickets available, then user can order tickets.&#13;
&#13;
1. user send order request (userid, tickets, event)&#13;
2. API gateway forward request to order server&#13;
3.  oder service integrates with 3rd party payment service like Strip for payment&#13;
4. order server&#13;
    -  query the availability of the tickets, and book them&#13;
    -  update the ticket status to booked once payment is done&#13;
    -  new order record is added in the same transaction.&#13;
&#13;
# Deep dive&#13;
&#13;
## Bad user experience when booking&#13;
&#13;
User find the available ticket and then pay for them, but found that it turns out to be ordered, the problem is that we don't reserve for the tickets.  &#13;
&#13;
To reserve the tickets, one way is to use DB feature like row level locking to lock the record, the problem is that it doesn't support timeout, and it's up to the application to handle the edge cases of the lock, to avoid leaving it to uncertain states, this helps on avoid double booking but it's not good to use it for reservation.&#13;
&#13;
Another way is to use the status field in DB, add reserved states, and use a cron job to check the expiration on timely basis, this may have a delay in unlocking depending on the interval of the cron job.&#13;
&#13;
A more preferred way is to use Redis, and it works as below:&#13;
&#13;
**Reservation**&#13;
1. User select available seat and book a ticket&#13;
2. Order service add  (ticket id, user id) in Redis Distributed Lock with reservation timeout, the record will be removed automatically once timeout TTL&#13;
3. Order service creates an order record and returns the order id&#13;
4. Other users need to check the availability in Redis before booking (available in DB and not reserved in Redis)&#13;
&#13;
**Payment**&#13;
1. with the order id , user can issue payment from the payment service&#13;
2. the payment service can callback order service once the payment succeeds, with the order id&#13;
3. order service write the payment detail in order record, and change the ticket status to 'booked'&#13;
&#13;
## How to scale up to tenth of millions of concurrent users during popular events&#13;
&#13;
To cope with read heavy view events up to tenth of millions:&#13;
&#13;
- we can utilize cache, since the view event is mostly unchanged&#13;
-  event service is stateless, so it can easily scale horizontally, and we can use load balance before the event services&#13;
&#13;
## How to handle millions of concurrent bookings during popular events&#13;
&#13;
First the user should be able to be notified that the ticket states immediately, polling won't work in this case, SSE(server send event) works for this case. But still the system may not able to handle such a kind of burst, and we need a mechanism to protect the system, we can add a toggled feature that could be enabled in this case, and park the users in a queue, and the client can use websocket to send request to the queuing service to get a token,  once order service deque a user from the queue, it can issue a token for the user, and notify the user via websocket to send the order request.&#13;
&#13;
![image](https://github.com/user-attachments/assets/17098f97-99c0-4c2e-8476-8aec4e550da4)&#13;
&#13;
## How to improve search to meet low latency requirements?&#13;
&#13;
SQL 'like' will scan the whole table, it's not efficient, we can either build full text index in DB, or use ElasticSearch with updates via CDC (Change Data Capture).&#13;
&#13;
The search results can be also cached with memcached or Redis, but could be more efficient to use ElasticSearch cache b/c it's smarter with app logic (reuse cache for aggregation or filtering etc). &#13;
&#13;
![image](https://github.com/user-attachments/assets/e68e60d6-1a22-48ba-b022-ca6553d29529)&#13;
。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20TicketMaster.html</guid><pubDate>Sat, 11 Jan 2025 01:12:43 +0000</pubDate></item><item><title>Change Data Capture (CDC)</title><link>https://leadtechinterview.github.io/post/Change%20Data%20Capture%20%28CDC%29.html</link><description>System design interviews often delve into the complexities of data synchronization and real-time data processing. One crucial technique that frequently surfaces is Change Data Capture (CDC). Understanding CDC is essential for designing robust and scalable systems. This post will introduce you to the core concepts of CDC and its importance in system design.&#13;
&#13;
# What is Change Data Capture (CDC)?&#13;
&#13;
Change Data Capture is a set of software design patterns used to determine and track data that has changed in a database so that action can be taken using the changed data. It's about efficiently capturing and propagating changes made to data in a source system (usually a database) to downstream systems in near real-time. This is crucial for various use cases, including:   &#13;
&#13;
- Keeping search indexes up-to-date (e.g., Elasticsearch, Solr): Ensuring search results reflect the latest data.&#13;
- Data warehousing and ETL (Extract, Transform, Load): Populating data warehouses with incremental changes.&#13;
- Real-time analytics and dashboards: Providing up-to-the-minute insights.&#13;
- Cache invalidation: Keeping caches consistent with the source data.&#13;
- Auditing and compliance: Tracking data changes for regulatory purposes.&#13;
&#13;
# Why is CDC Important in System Design?&#13;
&#13;
Traditional methods of data synchronization, like batch processing or polling, can be inefficient and introduce significant latency. &#13;
&#13;
CDC offers several advantages:&#13;
&#13;
- Near Real-Time Updates: CDC captures changes as they happen, enabling near real-time data synchronization.&#13;
- Reduced Database Load: Instead of constantly querying the database for changes, CDC reads change logs or uses other efficient mechanisms, minimizing the impact on database performance.&#13;
- Data Integrity: CDC ensures that changes are captured accurately and in order, maintaining data consistency across different systems.&#13;
- Decoupling: CDC decouples the source system from downstream systems, improving flexibility and scalability.&#13;
&#13;
# Common CDC Techniques:&#13;
&#13;
There are several ways to implement CDC, each with its own trade-offs:&#13;
&#13;
- Database Logs (Log-Based CDC): This is the most efficient and preferred method. Databases maintain transaction logs (e.g., redo logs in Oracle, write-ahead logs (WAL) in PostgreSQL, binary logs in MySQL) that record every change made to the database. CDC tools can parse these logs and extract the change data.&#13;
&#13;
Pros: Minimal impact on database performance, high throughput, reliable.&#13;
Cons: Requires access to database logs, can be complex to implement directly.&#13;
&#13;
- Triggers: Database triggers can be used to capture changes. A trigger is a stored procedure that automatically executes in response to certain events (e.g., INSERT, UPDATE, DELETE).&#13;
&#13;
Pros: Relatively simple to implement.&#13;
Cons: Can add overhead to database operations, can be difficult to manage complex change capture logic, not suitable for high-volume changes.&#13;
&#13;
- Polling: This involves periodically querying the database for changes based on a timestamp column or a modified flag.&#13;
&#13;
Pros: Simple to implement.&#13;
Cons: Inefficient, introduces latency, puts significant load on the database, can miss changes if the polling interval is too long.&#13;
&#13;
- Dual Writes: This involves writing data to both the database and the downstream system simultaneously.&#13;
&#13;
Pros: Simple for basic use cases&#13;
Cons: Introduces tight coupling, difficult to ensure atomicity, prone to inconsistencies if one write fails.&#13;
&#13;
# Example Scenario: Keeping Elasticsearch Synchronized&#13;
&#13;
Imagine a system with a relational database storing product information and Elasticsearch used for search. Using CDC, you would:&#13;
&#13;
- Use a tool like Debezium to capture changes from the database's transaction log.&#13;
- Stream these changes through a message queue like Kafka.&#13;
- Have a consumer application read from Kafka, transform the change data into a format suitable for Elasticsearch, and index it.&#13;
&#13;
This ensures that Elasticsearch is always up-to-date with the latest product information without constantly querying the database.&#13;
&#13;
Understanding CDC is a valuable asset in system design interviews. It demonstrates your knowledge of data synchronization techniques and your ability to design efficient and scalable systems. By understanding the different methods and their trade-offs, you can effectively address related questions and showcase your expertise.。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Change%20Data%20Capture%20%28CDC%29.html</guid><pubDate>Fri, 10 Jan 2025 07:40:57 +0000</pubDate></item><item><title>Avoid double booking</title><link>https://leadtechinterview.github.io/post/Avoid%20double%20booking.html</link><description># Problem&#13;
&#13;
Imagine two users try to buy the last ticket to a show at almost the same instant. Without a proper system, it's possible both users could be told they successfully bought the ticket, leading to overselling.&#13;
&#13;
# Solutions&#13;
&#13;
To ensure that no two users book the same ticket simultaneously, the Booking Service uses database transactions with ACID properties, employing techniques like row-level locking or optimistic concurrency control (OCC).&#13;
&#13;
- **Row-Level Locking**: This is one technique to achieve isolation. When a user starts booking a ticket, the database places a 'lock' on the specific row in the database table that represents that ticket. This lock prevents any other transaction from modifying that row until the first transaction is finished (either committed or rolled back). Think of it like putting a 'reserved' sign on the ticket.&#13;
&#13;
- **Optimistic Concurrency Control (OCC)**: This is an alternative to locking. Instead of locking the row, the system checks if the data has been modified by another transaction before committing the current transaction. It typically does this by comparing a version number or timestamp. If the data has changed in the meantime, the transaction is aborted, and the user might be informed that the ticket is no longer available. This is 'optimistic' because it assumes conflicts are rare.&#13;
&#13;
## Row-Level Locking&#13;
&#13;
Scenario: Imagine a database table storing airline seat reservations. Each row represents a seat on a specific flight.&#13;
&#13;
| Flight | Seat | Customer |&#13;
|---|---|---|&#13;
| AA123 | 1A | John Doe |&#13;
| AA123 | 1B | Jane Smith |&#13;
| AA123 | 2A |  |&#13;
| AA123 | 2B |  |&#13;
&#13;
Two users try to book seat 2A simultaneously:&#13;
&#13;
1. User 1 clicks to book seat 2A. The database transaction begins.&#13;
2. The database places a lock on the row representing seat 2A.&#13;
3. User 2, at the exact same moment, also clicks to book seat 2A. Their transaction also begins.&#13;
&#13;
However, because User 1's transaction has already locked the row, User 2's transaction is blocked. It has to wait.&#13;
User 1 completes their booking (payment goes through, etc.). The transaction commits, and the lock on seat 2A is released.&#13;
&#13;
Now, User 2's transaction can proceed. But when it tries to access the row for seat 2A, it sees that it's no longer available (John Doe has it). The system informs User 2 that the seat is taken.&#13;
&#13;
## Optimistic Concurrency Control (OCC)&#13;
&#13;
Scenario: A similar airline seat reservation system. This time, instead of locks, each row has a version number.&#13;
&#13;
| Flight | Seat | Customer | Version |&#13;
|---|---|---|---|&#13;
| AA123 | 1A | John Doe | 1 |&#13;
| AA123 | 1B | Jane Smith | 1 |&#13;
| AA123 | 2A |  | 1 |&#13;
| AA123 | 2B |  | 1 |&#13;
&#13;
Two users try to book seat 2A simultaneously:&#13;
&#13;
1. User 1 starts the booking process for seat 2A. The system reads the row and notes the version number (1).&#13;
2. User 2, at almost the same time, also starts booking seat 2A. Their system also reads the row and notes the version number (1).&#13;
3. User 1 completes their booking. The system checks if the version number in the database is still 1. It is, so the system updates the row with User 1's information and increments the version number to 2.&#13;
4. User 2 completes their booking a fraction of a second later. Their system also checks if the version number is still 1. But now, it's 2! This means the row has been modified by another transaction (User 1's).&#13;
5. User 2's transaction is aborted. The system informs them that the seat is no longer available.&#13;
&#13;
## Key Differences&#13;
&#13;
### Locking (Pessimistic):&#13;
&#13;
- Locks are acquired immediately, preventing conflicts upfront.&#13;
- Can lead to performance issues if there are many concurrent users trying to access the same data (because of waiting).&#13;
- Better for situations where conflicts are likely (e.g., very popular events).&#13;
&#13;
### OCC (Optimistic):&#13;
&#13;
- Assumes conflicts are rare and only checks for them at the end.&#13;
- Generally better performance for most applications because there's no waiting.&#13;
- More complex to implement because you need to handle the cases where transactions are aborted.&#13;
- Both methods are used to ensure data integrity in concurrent environments, but they have different trade-offs in terms of performance and complexity.&#13;
&#13;
# MySQL examples&#13;
&#13;
Here are some MySQL examples demonstrating row-level locking and how it would conceptually work (MySQL doesn't directly expose OCC in the same way). I'll also explain how OCC would be implemented in SQL conceptually.&#13;
&#13;
## Row-Level Locking (using FOR UPDATE)&#13;
&#13;
MySQL uses FOR UPDATE to acquire exclusive row-level locks.&#13;
&#13;
1. Table Setup:&#13;
&#13;
```SQL&#13;
CREATE TABLE tickets (&#13;
    id INT PRIMARY KEY AUTO_INCREMENT,&#13;
    event_name VARCHAR(255),&#13;
    available_seats INT&#13;
);&#13;
&#13;
INSERT INTO tickets (event_name, available_seats) VALUES ('Concert X', 10);&#13;
```&#13;
&#13;
2. Booking Process (simulating two concurrent users):&#13;
&#13;
- User 1 (in one MySQL session):&#13;
&#13;
```SQL&#13;
START TRANSACTION; -- Start a transaction&#13;
&#13;
SELECT available_seats FROM tickets WHERE id = 1 FOR UPDATE; -- Lock the row&#13;
&#13;
-- Check if seats are available&#13;
SET @seats := (SELECT available_seats FROM tickets WHERE id = 1);&#13;
IF @seats &gt; 0 THEN&#13;
    UPDATE tickets SET available_seats = available_seats - 1 WHERE id = 1;&#13;
    SELECT 'Booking successful' AS message;&#13;
    COMMIT; -- Commit the transaction, releasing the lock&#13;
ELSE&#13;
    SELECT 'No seats available' AS message;&#13;
    ROLLBACK; -- Rollback the transaction&#13;
END IF;&#13;
```&#13;
&#13;
- User 2 (in a separate MySQL session, running at almost the same time):&#13;
&#13;
```SQL&#13;
START TRANSACTION;&#13;
&#13;
SELECT available_seats FROM tickets WHERE id = 1 FOR UPDATE; -- This will block until User 1's transaction commits&#13;
&#13;
-- (Once User 1 commits, this continues)&#13;
SET @seats := (SELECT available_seats FROM tickets WHERE id = 1);&#13;
IF @seats &gt; 0 THEN&#13;
    UPDATE tickets SET available_seats = available_seats - 1 WHERE id = 1;&#13;
    SELECT 'Booking successful' AS message;&#13;
    COMMIT;&#13;
ELSE&#13;
    SELECT 'No seats available' AS message;&#13;
    ROLLBACK;&#13;
END IF;&#13;
```&#13;
&#13;
3. Explanation:&#13;
&#13;
&gt;START TRANSACTION begins a transaction.&#13;
SELECT ... FOR UPDATE acquires an exclusive lock on the selected row. This prevents other transactions from modifying the row until the current transaction is committed or rolled back.&#13;
If User 2 tries to execute the SELECT ... FOR UPDATE while User 1's transaction holds the lock, User 2's query will wait.&#13;
COMMIT makes the changes permanent and releases the lock.&#13;
ROLLBACK undoes any changes and releases the lock.&#13;
&#13;
## Conceptual OCC in SQL (using a version column)&#13;
&#13;
MySQL doesn't have built-in OCC like some other databases, but you can implement it yourself using a version column:&#13;
&#13;
1. Table Setup (with a version column):&#13;
&#13;
```SQL&#13;
ALTER TABLE tickets ADD COLUMN version INT UNSIGNED NOT NULL DEFAULT 0;&#13;
```&#13;
&#13;
2. Booking Process:&#13;
&#13;
```SQL&#13;
START TRANSACTION;&#13;
&#13;
SELECT available_seats, version FROM tickets WHERE id = 1 INTO @seats, @version;&#13;
&#13;
IF @seats &gt; 0 THEN&#13;
    UPDATE tickets &#13;
    SET available_seats = available_seats - 1, version = version + 1&#13;
    WHERE id = 1 AND version = @version; -- Check the version here!&#13;
&#13;
    IF ROW_COUNT() &gt; 0 THEN -- Check if the update was successful&#13;
        SELECT 'Booking successful' AS message;&#13;
        COMMIT;&#13;
    ELSE&#13;
        SELECT 'Booking failed (concurrent update)' AS message;&#13;
        ROLLBACK;&#13;
    END IF;&#13;
ELSE&#13;
    SELECT 'No seats available' AS message;&#13;
    ROLLBACK;&#13;
END IF;&#13;
```&#13;
&#13;
3. Explanation of OCC Implementation:&#13;
&#13;
We retrieve the available_seats and the version number in one SELECT.&#13;
The crucial part is the `WHERE id = 1 AND version = @version` clause in the UPDATE statement. This condition ensures that the update only happens if the version number in the database still matches the version number we read earlier.&#13;
ROW_COUNT() checks how many rows were affected by the UPDATE. If it's 0, it means the version has changed (another transaction modified the row), and the booking fails.&#13;
&#13;
This is a simplified example. In a real application, you would need to handle the 'booking failed' case gracefully, perhaps by retrying the booking or informing the user.&#13;
&#13;
These examples illustrate the core concepts of row-level locking and OCC in MySQL. They are simplified for clarity but demonstrate the key mechanisms involved. Remember that actual performance and behavior can be affected by factors like database configuration, indexes, and transaction isolation levels.&#13;
&#13;
&gt; [!NOTE]&#13;
&gt;  The behavior of row-level locking without FOR UPDATE depends heavily on the transaction isolation level set for your MySQL session or globally for the server.&#13;
&gt;&#13;
&gt; - **Default Isolation Level (REPEATABLE READ)**: In MySQL's default REPEATABLE READ isolation level, a simple SELECT statement within a transaction does not acquire any locks that prevent other transactions from modifying the selected rows. This means that if you perform a SELECT and then later try to UPDATE based on the data you read, you could encounter a 'lost update' problem.&#13;
&gt; - **READ COMMITTED Isolation Level**: In READ COMMITTED, a SELECT statement reads only committed data. However, it still doesn't acquire locks that prevent other transactions from modifying the data after the SELECT has finished. So, the lost update problem can still occur.&#13;
&gt; - **SERIALIZABLE Isolation Level**: This is the highest isolation level. In SERIALIZABLE, even a simple SELECT statement acquires shared locks that prevent other transactions from modifying the selected rows. This prevents lost updates and other concurrency problems, but it can also significantly reduce concurrency and performance.&#13;
&gt; - **Using FOR UPDATE (Pessimistic Locking)**: As discussed before, FOR UPDATE explicitly acquires an exclusive lock on the selected rows, regardless of the transaction isolation level (except in some very specific edge cases related to storage engines). This is the most reliable way to prevent concurrency issues like lost updates when you need to update data based on a previous read.。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Avoid%20double%20booking.html</guid><pubDate>Fri, 10 Jan 2025 05:47:49 +0000</pubDate></item><item><title>System Design Delivery Framework</title><link>https://leadtechinterview.github.io/post/System%20Design%20Delivery%20Framework.html</link><description># Recommended Structure:&#13;
&#13;
1. Requirements (5 mins):&#13;
&#13;
- Functional Requirements: Identify core features (e.g., 'Users should be able to post tweets'). Prioritize 2-3 key features.&#13;
&#13;
- Non-Functional Requirements: Focus on system qualities like scalability, latency, and availability. Quantify where possible (e.g., 'render feeds in under 200ms').&#13;
&#13;
2. Capacity Estimation: Skip unnecessary calculations unless they directly impact the design (e.g., sharding in a TopK system).&#13;
&#13;
3. Core Entities (2 mins): Identify key entities (e.g., User, Tweet, Follow) to define the system's foundation.&#13;
&#13;
4. API/System Interface (5 mins): Define the contract between the system and users. Prefer RESTful APIs unless GraphQL is necessary.&#13;
&#13;
5. [Optional] Data Flow (5 mins): Describe high-level processes for data-heavy systems (e.g., web crawlers).&#13;
&#13;
6. High-Level Design (10-15 mins): Draw the system architecture, focusing on core components (e.g., servers, databases). Keep it simple and iterate based on API endpoints.&#13;
&#13;
7. Deep Dives (10 mins): Address non-functional requirements, edge cases, and bottlenecks. Proactively improve the design (e.g., scaling, caching, database sharding).&#13;
&#13;
![flow chart](https://www.mermaidchart.com/raw/580407c4-2ada-4e23-9a4b-fa8e09f3963a?theme=light&amp;version=v0.1&amp;format=svg)&#13;
&#13;
# Tips:&#13;
&#13;
1. Avoid overcomplicating the design early on.&#13;
&#13;
2. Communicate clearly with the interviewer, explaining your thought process and data flow.&#13;
&#13;
3. Focus on relevant fields in the data model, not every detail.&#13;
&#13;
4. Balance proactive discussion with listening to the interviewer’s probes.&#13;
&#13;
# Example: Twitter System Design&#13;
&#13;
1. Functional Requirements: Post tweets, follow users, view feeds.&#13;
&#13;
2. Non-Functional Requirements: High availability, low latency (&lt;200ms), scalability to 100M+ DAUs.&#13;
&#13;
3. Core Entities: User, Tweet, Follow.&#13;
&#13;
4. API Endpoints: POST /v1/tweet, GET /v1/feed, etc.&#13;
&#13;
5. Deep Dives: Discuss fanout-on-read vs. fanout-on-write, caching, and horizontal scaling.。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/System%20Design%20Delivery%20Framework.html</guid><pubDate>Thu, 09 Jan 2025 17:56:45 +0000</pubDate></item><item><title>Creating Back of Envelope Quizzes for Tech Interviews using AI</title><link>https://leadtechinterview.github.io/post/Creating%20Back%20of%20Envelope%20Quizzes%20for%20Tech%20Interviews%20using%20AI.html</link><description># Prompt&#13;
&#13;
would suggest use https://chat.deepseek.com, looks like it does a better job than chatGPT o1 mini on the math problems.&#13;
&#13;
&gt; You are a quiz creator of highly diagnostic quizzes. You will look up how to develop low-stakes tests and diagnostics. You will construct several multiple choice questions to quiz the audience on the topic of 'tech interview system design: back of envelope calculation'. The questions should be highly relevant and go beyond just facts. Multiple choice questions should include plausible, competitive alternate responses and should not include an 'all of the above option.' At the end of the quiz, you will provide an answer key and explain the right answer.&#13;
&#13;
# Quizzes&#13;
&#13;
Here’s a highly diagnostic quiz on the topic of **'Tech Interview System Design: Back-of-the-Envelope Calculations'**. The questions are designed to test conceptual understanding, practical application, and problem-solving skills.&#13;
&#13;
---&#13;
&#13;
### **Quiz: Back-of-the-Envelope Calculations in System Design**&#13;
&#13;
#### **Question 1: Estimating Request Throughput**&#13;
You are designing a social media platform and need to estimate the number of requests per second (RPS) the system must handle. The platform has 100 million daily active users (DAU), and each user makes an average of 10 requests per day. What is the approximate RPS the system must handle?&#13;
&#13;
A) 1,000 RPS  &#13;
B) 10,000 RPS  &#13;
C) 100,000 RPS  &#13;
D) 1,000,000 RPS  &#13;
&#13;
---&#13;
&#13;
#### **Question 2: Storage Requirements for a Photo-Sharing App**&#13;
A photo-sharing app allows users to upload 1 photo per day. Each photo is approximately 5 MB in size. If the app has 10 million users, how much storage is required per year?&#13;
&#13;
A) 18 TB  &#13;
B) 180 TB  &#13;
C) 1.8 PB  &#13;
D) 18 PB  &#13;
&#13;
---&#13;
&#13;
#### **Question 3: Latency Calculation**&#13;
A distributed system has a processing latency of 50 ms per request and a network latency of 100 ms per request. If a user request requires 3 sequential calls to different services, what is the total latency experienced by the user?&#13;
&#13;
A) 150 ms  &#13;
B) 300 ms  &#13;
C) 450 ms  &#13;
D) 600 ms  &#13;
&#13;
---&#13;
&#13;
#### **Question 4: Cache Sizing for a News Feed**&#13;
A news feed system serves 1 million requests per hour, and each request fetches 10 KB of data. If the cache hit rate is 50%, how much data is served from the cache per hour?&#13;
&#13;
A) 5 GB  &#13;
B) 10 GB  &#13;
C) 20 GB  &#13;
D) 50 GB  &#13;
&#13;
---&#13;
&#13;
#### **Question 5: Bandwidth Calculation for Video Streaming**&#13;
A video streaming platform serves 1 million concurrent users. Each user streams video at 5 Mbps. What is the total bandwidth required to support this load?&#13;
&#13;
A) 5 Gbps  &#13;
B) 50 Gbps  &#13;
C) 500 Gbps  &#13;
D) 5 Tbps  &#13;
&#13;
---&#13;
&#13;
### **Answer Key and Explanations**&#13;
&#13;
#### **Question 1: Estimating Request Throughput**&#13;
**Correct Answer: B) 10,000 RPS**  &#13;
Explanation:  &#13;
- Total requests per day = 100 million DAU * 10 requests/user = 1 billion requests/day.  &#13;
- Requests per second = 1 billion / (24 * 60 * 60) ≈ 11,574 RPS.  &#13;
- The closest option is 10,000 RPS.&#13;
&#13;
---&#13;
&#13;
#### **Question 2: Storage Requirements for a Photo-Sharing App**&#13;
**Correct Answer: C) 1.8 PB**  &#13;
Explanation:  &#13;
- Total photos per year = 10 million users * 1 photo/user/day * 365 days = 3.65 billion photos.  &#13;
- Total storage = 3.65 billion * 5 MB = 18.25 million GB = 18.25 PB.  &#13;
- The closest option is 1.8 PB (note: this is a trick question to test unit conversion).&#13;
&#13;
---&#13;
&#13;
#### **Question 3: Latency Calculation**&#13;
**Correct Answer: C) 450 ms**  &#13;
Explanation:  &#13;
- Total latency = (processing latency + network latency) * number of sequential calls.  &#13;
- Total latency = (50 ms + 100 ms) * 3 = 450 ms.&#13;
&#13;
---&#13;
&#13;
#### **Question 4: Cache Sizing for a News Feed**&#13;
**Correct Answer: A) 5 GB**  &#13;
Explanation:  &#13;
- Total data served per hour = 1 million requests * 10 KB = 10 GB.  &#13;
- Cache hit rate is 50%, so 50% of the data is served from the cache.  &#13;
- Data served from cache = 10 GB * 0.5 = 5 GB.&#13;
&#13;
---&#13;
&#13;
#### **Question 5: Bandwidth Calculation for Video Streaming**&#13;
**Correct Answer: D) 5 Tbps**  &#13;
Explanation:  &#13;
- Total bandwidth = 1 million users * 5 Mbps = 5 million Mbps = 5 Tbps.  &#13;
- This tests your ability to convert units (Mbps to Tbps).&#13;
&#13;
---&#13;
&#13;
This quiz is designed to test your ability to perform back-of-the-envelope calculations, which are critical for system design interviews. Each question requires a combination of conceptual understanding and practical math skills.。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Creating%20Back%20of%20Envelope%20Quizzes%20for%20Tech%20Interviews%20using%20AI.html</guid><pubDate>Thu, 09 Jan 2025 16:58:29 +0000</pubDate></item><item><title>Simplified Back-of-the-Envelope Calculation Cheat Sheet</title><link>https://leadtechinterview.github.io/post/Simplified%20Back-of-the-Envelope%20Calculation%20Cheat%20Sheet.html</link><description>### **Simplified Back-of-the-Envelope Calculation Cheat Sheet**&#13;
&#13;
| **Category**         | **Metric**                          | **Value**                     | **Notes**                                                                 |&#13;
|-----------------------|-------------------------------------|-------------------------------|---------------------------------------------------------------------------|&#13;
| **Time**             | 1 second                           | 1,000 milliseconds (ms)       | Useful for latency calculations.                                         |&#13;
|                       | 1 day                              | ~100,000 seconds              | Rounded up for easier estimation.                                        |&#13;
| **Data Size**        | 1 kilobyte (KB)                    | 10&lt;sup&gt;3&lt;/sup&gt; bytes          | ~1,000 bytes (thousand).                                                 |&#13;
|                       | 1 megabyte (MB)                    | 10&lt;sup&gt;6&lt;/sup&gt; bytes          | ~1,000 KB (million).                                                     |&#13;
|                       | 1 gigabyte (GB)                    | 10&lt;sup&gt;9&lt;/sup&gt; bytes          | ~1,000 MB (billion).                                                     |&#13;
|                       | 1 terabyte (TB)                    | 10&lt;sup&gt;12&lt;/sup&gt; bytes         | ~1,000 GB (trillion).                                                    |&#13;
| **Network**          | Bandwidth of 1 Gbps                | 125 MB/s                      | 1 Gbps = 1,000 Mbps = 125 MB/s (divide by 8 to convert bits to bytes).   |&#13;
|                       | Round-trip time (RTT)              | ~100 ms (within a region)     | Assumes low latency within a data center or region.                      |&#13;
| **Storage**          | SSD latency                        | ~0.1 ms (100 μs)              | Fast read/write times for SSDs.                                          |&#13;
|                       | HDD latency                        | ~10 ms                        | Slower than SSDs but cheaper for bulk storage.                           |&#13;
| **Throughput**       | Requests per second (RPS)          | ~1,000 RPS per server         | Depends on server capacity and workload.                                 |&#13;
|                       | Queries per second (QPS)           | ~10,000 QPS per database      | Depends on database type and optimization.                               |&#13;
| **Memory**           | RAM access time                    | ~100 ns                       | Much faster than disk access.                                            |&#13;
|                       | Cache access time (L1)             | ~1 ns                         | Extremely fast access for frequently used data.                          |&#13;
| **Users**            | Daily Active Users (DAU)           | ~10% of total users           | Assumes 10% of users are active daily.                                   |&#13;
|                       | Monthly Active Users (MAU)         | ~30% of total users           | Assumes 30% of users are active monthly.                                 |&#13;
| **Traffic**          | Reads vs. Writes                   | ~90% reads, 10% writes        | Common for read-heavy systems (e.g., social media).                      |&#13;
|                       | Peak traffic multiplier            | ~2x to 10x average traffic    | Plan for peak traffic spikes (e.g., Black Friday).                       |&#13;
| **Miscellaneous**    | UUID size                          | 128 bits (16 bytes)           | Unique identifier size.                                                  |&#13;
|                       | Compression ratio                  | ~2x to 10x                    | Depends on data type (e.g., text compresses better than images).         |&#13;
&#13;
---&#13;
&#13;
### **How to Use This Table**&#13;
1. **Estimate Traffic**: Use DAU/MAU and peak traffic multipliers to estimate requests per second.&#13;
2. **Calculate Bandwidth**: Convert between bits and bytes to estimate network throughput.&#13;
3. **Compare Latencies**: Use SSD/HDD/RAM latencies to decide storage and caching strategies.&#13;
4. **Size Data**: Use data size conversions to estimate storage requirements.&#13;
5. **Plan for Scale**: Use RPS/QPS estimates to determine the number of servers or databases needed.&#13;
&#13;
---&#13;
&#13;
### **Example Calculation**&#13;
- **Scenario**: You’re designing a system with 1 million DAU, and each user makes 10 requests per day.&#13;
  - Total requests per day = 1,000,000 × 10 = 10,000,000 requests/day.&#13;
  - Requests per second (RPS) = 10,000,000 / 100,000 ≈ 100 RPS.&#13;
  - Peak traffic = 100 × 5 (assume 5x multiplier) ≈ 500 RPS.&#13;
  - If each server handles 1,000 RPS, you’ll need ~1 server (with room for growth).。</description><guid isPermaLink="true">https://leadtechinterview.github.io/post/Simplified%20Back-of-the-Envelope%20Calculation%20Cheat%20Sheet.html</guid><pubDate>Thu, 09 Jan 2025 02:08:28 +0000</pubDate></item></channel></rss>